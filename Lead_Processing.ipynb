{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5766fccd842e47c18cd481a97965700b",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "v6dipGZxQR65"
      },
      "source": [
        "### Install & import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "5cfed88fa52e41b99eeabe874853242c",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 8494,
        "execution_start": 1700255263606,
        "source_hash": null,
        "id": "BoiUE40PQR68"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade openai --quiet\n",
        "!pip install --upgrade pymongo --quiet\n",
        "!pip install --upgrade email-validator --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "ca0e30934e464635861bf2818e5a8a6b",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 807,
        "execution_start": 1700255413126,
        "source_hash": null,
        "id": "9ic5pgA6QR69"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import email_validator\n",
        "import warnings\n",
        "\n",
        "from openai import OpenAI\n",
        "from pymongo import MongoClient\n",
        "from urllib.request import urlopen\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5530c24a8aa2444cb5ddf60a4493081b",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "YCk0zANaQR69"
      },
      "source": [
        "### Read in CSVs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "3ee5aabb4ed74fff85d5db2deb4eb775",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [
          {
            "fromCodePoint": 25,
            "marks": {
              "code": true
            },
            "toCodePoint": 33,
            "type": "marks"
          },
          {
            "fromCodePoint": 38,
            "marks": {
              "code": true
            },
            "toCodePoint": 48,
            "type": "marks"
          }
        ],
        "id": "xvArCYR8QR6-"
      },
      "source": [
        "We'll read in two CSVs - AllLeads and human_data which are two CSVs of leads. The first contains leads from a run by the Lead Automator programmatic tool, while the other contains leads collected manually by the 10K team."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cell_id": "c4ebe1fa2deb411caabe25f57656dfb0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 30326,
        "execution_start": 1700256213606,
        "source_hash": null,
        "id": "wrPkF6YpQR6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "027d410e-dcf7-4060-db0c-b14a17aa2f96"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: ''",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-660671532fae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mleads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Enter dataset URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ],
      "source": [
        "# Read in CSV\n",
        "leads = pd.read_csv(\"\") # Enter dataset URL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0f43eb5f61fa4dba9a07c9b022ce9b23",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "yG99a_czQR6-"
      },
      "source": [
        "### Define Variables + Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "c8cf6e8f160542488adfcef2cff42356",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [
          {
            "fromCodePoint": 133,
            "marks": {
              "code": true
            },
            "toCodePoint": 148,
            "type": "marks"
          }
        ],
        "id": "qd5ilBJ_QR6-"
      },
      "source": [
        "Here, we'll define constants, initialize clients, and create all the sub-functions for processing leads that we will use in the main process_leads() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "f914ec26000a43c381bbd82e33d15301",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 147,
        "execution_start": 1700255419359,
        "source_hash": null,
        "id": "eA-tYq1ZQR6-"
      },
      "outputs": [],
      "source": [
        "# Define constants for functions\n",
        "NO_NAMES_INDICATOR = \"School Contact\"\n",
        "INVALID_NAME_TERMS = [\"school\", \"department\"]\n",
        "BUCKETS = [\"Sports\", \"Auxiliary\", \"Instructor\", \"Stem\", \"Administration\", \"Advisory\", \"Clerical\", \"Counseling\"]\n",
        "PREFIXES = [\"dr\", \"ms\", \"mr\", \"mrs\", \"miss\"]\n",
        "\n",
        "# Define output files for logging and data\n",
        "LOG_FILE = \"LeadProcessingLog.txt\"\n",
        "OUTPUT_FILE = \"AllLeadsFiltered.csv\"\n",
        "\n",
        "prompt_bytes_string = urlopen(\"https://raw.githubusercontent.com/bamartin1618/p4_data/main/LeadProcessingPrompt.txt\").read()\n",
        "PROMPT = prompt_bytes_string.decode(\"utf-8\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\" # TODO: Replace with your API key\n",
        "client = OpenAI()\n",
        "\n",
        "# Initialize email validation testing\n",
        "resolver = email_validator.caching_resolver(timeout=10)\n",
        "email_validator.CHECK_DELIVERABILITY = True\n",
        "\n",
        "# Processing Functions\n",
        "def collect_metadata(df, style):\n",
        "    \"\"\"\n",
        "    Collects metadata as a JSON-formatted dictionary.\n",
        "    ARGS:\n",
        "        - df: DataFrame\n",
        "        - style: string, either \"NULL\" or \"ROLE\"\n",
        "    \"\"\"\n",
        "\n",
        "    if style == \"NULL\":\n",
        "\n",
        "        info = round((df.isnull().sum() / df.shape[0]) * 100, 2)\n",
        "        info = info[info > 0]\n",
        "        as_dict = info.to_dict()\n",
        "\n",
        "    elif style == \"ROLE\":\n",
        "\n",
        "        by_role = df.groupby(\"Bucket\").size()\n",
        "        by_role = round((by_role / by_role.sum()) * 100, 2)\n",
        "\n",
        "        as_dict = by_role.to_dict()\n",
        "\n",
        "    return as_dict\n",
        "\n",
        "\n",
        "def validate_email(email):\n",
        "    \"\"\"\n",
        "    Validates an email address by regex and invalid chars.\n",
        "    ARGS:\n",
        "        - email: string\n",
        "    \"\"\"\n",
        "\n",
        "    email_cleaned = email.replace('\"', \"\")\n",
        "    email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,3}\"\n",
        "\n",
        "    correct_email_structure = bool(re.match(email_pattern, email_cleaned)) and \"webmaster\" not in email_cleaned # Sysadmin email\n",
        "\n",
        "    if correct_email_structure:\n",
        "\n",
        "      try:\n",
        "\n",
        "        validity = email_validator.validate_email(email, dns_resolver=resolver)\n",
        "        email_normalized = validity.normalized\n",
        "        return email_normalized\n",
        "\n",
        "      except email_validator.EmailNotValidError:\n",
        "        return \"invalid_email\"\n",
        "\n",
        "    return \"invalid_email\"\n",
        "\n",
        "def bucket_roles(roles, count):\n",
        "    \"\"\"\n",
        "    Bucket role into one of 8 categories: Sports, Auxiliary, Stem, Instructor, Administration, Advisory, Clerical, Counseling.\n",
        "    ARGS:\n",
        "      - roles: String list of 10 roles\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        chat = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": PROMPT.format(count)},\n",
        "                {\"role\": \"user\", \"content\": roles}\n",
        "            ],\n",
        "        )\n",
        "\n",
        "        chat_response = chat.choices[0].message.content\n",
        "        list_of_buckets = chat_response.split(\";\")\n",
        "\n",
        "    except requests.exceptions.Timeout:\n",
        "        cleaned_buckets = [None] * 10\n",
        "        print(\"Request timed out.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        cleaned_buckets = [None] * 10\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "    cleaned_buckets = []\n",
        "    for messy_bucket in list_of_buckets:\n",
        "\n",
        "        clean = messy_bucket.strip()\n",
        "        if clean not in BUCKETS:\n",
        "\n",
        "            clean = \"Unknown\"\n",
        "\n",
        "        cleaned_buckets.append(clean)\n",
        "\n",
        "    if len(cleaned_buckets) != count:\n",
        "\n",
        "      # If GPT gave less\n",
        "      if len(cleaned_buckets) < count:\n",
        "\n",
        "        cleaned_buckets += [\"Unknown\"] * (count - len(cleaned_buckets))\n",
        "\n",
        "      # If GPT gave more\n",
        "      else:\n",
        "\n",
        "        cleaned_buckets = cleaned_buckets[:count]\n",
        "\n",
        "    return cleaned_buckets\n",
        "\n",
        "def process_chunk(df):\n",
        "    \"\"\"\n",
        "    Role buckets a set of 10 roles.\n",
        "    ARGS:\n",
        "      - df: A set of 10 rows from the original dataframe\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Collect all roles as a String \"Role_1, Role_2, ...\"\n",
        "    role_string = \";\".join(df[\"Role\"].tolist())\n",
        "    count = len(df[\"Role\"].tolist())\n",
        "\n",
        "    # 2. GPT it and get a response\n",
        "    buckets = bucket_roles(role_string, count)\n",
        "\n",
        "    # 3. Place them back in the column\n",
        "    df[\"Bucket\"] = buckets\n",
        "    return df\n",
        "\n",
        "def clean_names(row, indicator):\n",
        "    \"\"\"\n",
        "    Cleans the first and last names of a row.\n",
        "    ARGS:\n",
        "        - row: Dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    first_name = row[\"FirstName\"]\n",
        "    last_name = row[\"LastName\"]\n",
        "\n",
        "    if indicator == \"first\":\n",
        "\n",
        "        case = \"None\"\n",
        "\n",
        "        if type(first_name) != str:\n",
        "            return first_name, case\n",
        "\n",
        "        if not pd.isna(first_name):\n",
        "\n",
        "            first_name = first_name.lower().strip()\n",
        "            first_name = re.sub(r'[^a-zA-Z ]', '', first_name)\n",
        "\n",
        "            # Define conditions for invalid first name\n",
        "            is_initial_only = len(first_name) == 1\n",
        "            is_prefix = first_name in PREFIXES\n",
        "\n",
        "            if is_initial_only or is_prefix:\n",
        "                case = \"Initial / Prefix\"\n",
        "                row[\"FirstName\"] = np.NaN\n",
        "            else:\n",
        "\n",
        "                # If multiple first-names without hyphen\n",
        "                split_on_space = first_name.split(\" \")\n",
        "                if len(split_on_space) > 1:\n",
        "\n",
        "                    option_1 = split_on_space[0].lower().replace(\".\", \"\").strip()\n",
        "                    option_2 = split_on_space[1].lower().replace(\".\", \"\").strip()\n",
        "\n",
        "                    # Define conditions for invalid name pt.2\n",
        "                    contains_prefixes = option_1 in PREFIXES\n",
        "\n",
        "                    if contains_prefixes:\n",
        "                        case = \"Prefix / Multiple Names\"\n",
        "                        row[\"FirstName\"] = option_2.capitalize()\n",
        "                    else:\n",
        "                        case = \"Multiple Names\"\n",
        "                        row[\"FirstName\"] = option_1.capitalize()\n",
        "\n",
        "                else:\n",
        "\n",
        "                    row[\"FirstName\"] = first_name.capitalize()\n",
        "\n",
        "        return row[\"FirstName\"], case\n",
        "\n",
        "    else:\n",
        "\n",
        "        case = \"None\"\n",
        "        if type(last_name) != str:\n",
        "            return last_name, case\n",
        "\n",
        "        last_name = last_name.lower().strip()\n",
        "\n",
        "        # Define conditions for invalid last name\n",
        "        contains_invalid_terms = False\n",
        "        for term in INVALID_NAME_TERMS:\n",
        "            if last_name.count(term) > 0:\n",
        "                contains_invalid_terms = True\n",
        "\n",
        "        contains_prefixes = last_name in PREFIXES\n",
        "\n",
        "        if contains_invalid_terms:\n",
        "            case = \"Invalid Terms\"\n",
        "            row[\"LastName\"] = np.NaN\n",
        "        elif contains_prefixes:\n",
        "            case = \"Prefix\"\n",
        "            row[\"LastName\"] = row[\"FirstName\"].capitalize()\n",
        "            row[\"FirstName\"] = np.NaN\n",
        "        else:\n",
        "            row[\"LastName\"] = last_name.capitalize()\n",
        "\n",
        "\n",
        "        # If multiple last names, capitalize each\n",
        "\n",
        "        if not pd.isna(row[\"LastName\"]):\n",
        "            if \"-\" in last_name:\n",
        "                factor = \"-\"\n",
        "            else:\n",
        "                factor = \" \"\n",
        "\n",
        "            names = row[\"LastName\"].split(factor)\n",
        "            last_name = [name.capitalize() for name in names]\n",
        "            row[\"LastName\"] = factor.join(last_name)\n",
        "\n",
        "        return row[\"LastName\"], case\n",
        "\n",
        "\n",
        "def process(df, process):\n",
        "    \"\"\"\n",
        "    Processes the leads CSV collected by the Lead Automator.\n",
        "    ARGS:\n",
        "        - df: DataFrame\n",
        "        - process: string\n",
        "    \"\"\"\n",
        "\n",
        "    # Grab current size\n",
        "    current_size = df.shape[0]\n",
        "\n",
        "    # Various processses\n",
        "    if process == \"Nulls\":\n",
        "\n",
        "        df = df.dropna(subset=[\"Email\"])\n",
        "        df = df.dropna(subset=[\"FirstName\", \"LastName\"], how=\"all\")\n",
        "        df = df.dropna(subset=[\"LastName\"])\n",
        "        df = df.loc[(df[\"FirstName\"] != NO_NAMES_INDICATOR) & (df[\"LastName\"] != NO_NAMES_INDICATOR)]\n",
        "\n",
        "    elif process == \"Duplicates\":\n",
        "\n",
        "        df = df.drop_duplicates(subset=[\"Email\"])\n",
        "\n",
        "    elif process == \"Validity\":\n",
        "\n",
        "        df[\"Email\"] = df[\"Email\"].apply(validate_email)\n",
        "        df = df[df[\"Email\"] != \"invalid_email\"]\n",
        "\n",
        "    elif process == \"Roles\":\n",
        "\n",
        "        with_role = df.dropna(subset=[\"Role\"])\n",
        "        without_role = df[df[\"Role\"].isnull()]\n",
        "\n",
        "        chunk_size = 7\n",
        "        chunks = [with_role.iloc[i:i + chunk_size] for i in range(0, len(with_role), chunk_size)]\n",
        "\n",
        "        # Processing each chunk and storing the results\n",
        "        processed_chunks = [process_chunk(chunk) for chunk in chunks]\n",
        "        df = pd.concat(processed_chunks, ignore_index=True)\n",
        "\n",
        "        df = df[~df[\"Bucket\"].isin([\"Sports\", \"Auxiliary\", \"Clerical\"])]\n",
        "\n",
        "        role_replace_dict = {np.NaN: \"Faculty Contact\"}\n",
        "\n",
        "        without_role[\"Bucket\"] = [None for i in range(without_role.shape[0])]\n",
        "\n",
        "        df = pd.concat([df, without_role], ignore_index=True)\n",
        "        df[\"Role\"] = df[\"Role\"].replace(role_replace_dict)\n",
        "\n",
        "    elif process == \"Names\":\n",
        "\n",
        "        # TODO: Implement logging for case\n",
        "\n",
        "        df[[\"FirstName\", \"FirstNameCase\"]] = df.apply(clean_names, axis=1, indicator=\"first\", result_type=\"expand\")\n",
        "        df[[\"LastName\", \"LastNameCase\"]] = df.apply(clean_names, axis=1, indicator=\"last\", result_type=\"expand\")\n",
        "\n",
        "        df = df[~df[\"LastName\"].isnull()]\n",
        "\n",
        "    # Grab new size\n",
        "    new_size = df.shape[0]\n",
        "\n",
        "    impact = {process: current_size - new_size}\n",
        "    return df, impact"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ea0e837cbe8b4f13abedc344a55a9ae4",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "NIdQfgHMQR6_"
      },
      "source": [
        "### Process Leads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "4dfc4c13f15c4c92af2d75c7ee67e4ac",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "d6Jfsa6hQR6_"
      },
      "source": [
        "Using all the functions we've defined above, we'll process our leads to convert our messy input data to new clean and actionable data. We'll collect and print statistics while doing so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "8bf614cdf3fb4ef49e95eb4e57f67bc0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 1514,
        "execution_start": 1700255426069,
        "source_hash": null,
        "id": "iEQxmU3hQR6_"
      },
      "outputs": [],
      "source": [
        "# Primary Processing Function\n",
        "def process_leads(df):\n",
        "    \"\"\"\n",
        "    Processes the leads CSV collected by the Lead Automator.\n",
        "    ARGS:\n",
        "        - df: DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    with open(LOG_FILE, \"w\") as log:\n",
        "\n",
        "        log.write(\"Processing leads...\\n\")\n",
        "\n",
        "        # Grab current size\n",
        "        log.write(\"Total unprocessed leads before processing: {}\".format(df.shape[0]))\n",
        "\n",
        "        # Collect metadata\n",
        "        null_metadata = collect_metadata(df, \"NULL\")\n",
        "        log.write(\"\\nMetadata for NULL leads: \" + json.dumps(null_metadata))\n",
        "\n",
        "        # Convert \"Unknown\" and \"\" to NaN\n",
        "        replace_dict = {\"Unknown\": np.nan, \"\": np.nan}\n",
        "        df = df.replace(replace_dict)\n",
        "\n",
        "        # Process leads\n",
        "        df, impact = process(df, \"Nulls\")\n",
        "        log.write(\"\\nNumber of NULL leads: \" + json.dumps(impact))\n",
        "\n",
        "        df, impact = process(df, \"Validity\")\n",
        "        log.write(\"\\nNumber of Invalid leads: \" + json.dumps(impact))\n",
        "\n",
        "        df, impact = process(df, \"Duplicates\")\n",
        "        log.write(\"\\nNumber of Duplicate leads: \" + json.dumps(impact))\n",
        "\n",
        "        df, impact = process(df, \"Roles\")\n",
        "        log.write(\"\\nNumber of invalid roles: \" + json.dumps(impact))\n",
        "\n",
        "        df, impact = process(df, \"Names\")\n",
        "        log.write(\"\\nNumber of invalid names: \" + json.dumps(impact))\n",
        "\n",
        "        first_name_case_distributions = (df[\"FirstNameCase\"].value_counts() / df[\"FirstNameCase\"].value_counts().sum()).to_dict()\n",
        "        last_name_case_distributions = (df[\"LastNameCase\"].value_counts() / df[\"LastNameCase\"].value_counts().sum()).to_dict()\n",
        "\n",
        "        log.write(\"\\nDistribution Of First Name Cases: \" + json.dumps(first_name_case_distributions))\n",
        "        log.write(\"\\nDistribution Of Last Name Cases: \" + json.dumps(last_name_case_distributions))\n",
        "\n",
        "        role_metadata = collect_metadata(df, \"ROLE\")\n",
        "        log.write(\"\\nMetadata for role buckets: \" + json.dumps(role_metadata))\n",
        "\n",
        "        # Grab new size\n",
        "        log.write(\"\\nTotal leads after processing: {}\".format(df.shape[0]))\n",
        "        log.write(\"\\nFinished processing leads.\\n\")\n",
        "\n",
        "    # Drop unneeded columns\n",
        "    unhelpful_columns = [\"FirstNameCase\", \"LastNameCase\"]\n",
        "    df = df.drop(columns = unhelpful_columns)\n",
        "\n",
        "    # Write new leads to CSV\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "process_leads(leads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "5aa134783dfd4a599221d4b5a5ffa59c",
        "deepnote_cell_type": "text-cell-h3",
        "formattedRanges": [],
        "id": "WC_RMGv-QR7A"
      },
      "source": [
        "### Calculate Quality of Leads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "50371117b5e744399284b40e86422488",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "hOPr19jVQR7A"
      },
      "source": [
        "After processing, it's a good idea to get a numerical value of how \"perfect\" each lead is. Currently, we'll define a perfect lead as so:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "db91f7625e524ceab33bb2805e875689",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "KX8RR08IQR7A"
      },
      "source": [
        "1. The FirstName value is NOT blank, meaning the contact has a viable First and Last Name. (Contacts with an invalid / blank last name are filtered out earlier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "ee0bde54639e49328bf34211c5e5ad85",
        "deepnote_cell_type": "text-cell-p",
        "formattedRanges": [],
        "id": "qEuUX93eQR7A"
      },
      "source": [
        "2. The Bucket value is NOT \"Unknown\", meaning the contacts \"Role\" is something that GPT could determine a viable Bucket for, meaning it's a realistic Role value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "18be10b3d1b44477bdd7f329e84fd018",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 662,
        "execution_start": 1700179580186,
        "source_hash": null,
        "id": "wT174FJHQR7A"
      },
      "outputs": [],
      "source": [
        "# Write code to calculate \"quality\" of leads\n",
        "\n",
        "MAX_QUALITY = 2 # 2\n",
        "\n",
        "def calculate_quality(row):\n",
        "    \"\"\"\n",
        "    Calculates the quality of a lead.\n",
        "    ARGS:\n",
        "        - row: Dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    quality_score = 0\n",
        "\n",
        "    # If we have a first name\n",
        "    if not pd.isna(row[\"FirstName\"]):\n",
        "        quality_score += 1\n",
        "\n",
        "    # If we have a role that is bucketable\n",
        "    no_bucket_exists = pd.isna(row[\"Bucket\"])\n",
        "    bucket_is_unknown = row[\"Bucket\"] == \"Unknown\"\n",
        "\n",
        "\n",
        "    if (not no_bucket_exists) or (not bucket_is_unknown):\n",
        "        quality_score += 1\n",
        "\n",
        "    return quality_score\n",
        "\n",
        "leads_filtered = pd.read_csv(OUTPUT_FILE)\n",
        "leads_filtered[\"Quality\"] = leads_filtered.apply(calculate_quality, axis=1)\n",
        "\n",
        "with open(LOG_FILE, \"a\") as log:\n",
        "    perfect_lead_percentage = leads_filtered[leads_filtered[\"Quality\"] == MAX_QUALITY].shape[0] / leads_filtered.shape[0]\n",
        "    perfect_lead_percentage = round(perfect_lead_percentage * 100, 2)\n",
        "\n",
        "    log.write(\"\\nQuality of filtered leads: {}\".format(perfect_lead_percentage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cell_id": "877da1effd51494aa1dc7a1e561f7e43",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution_millis": 17,
        "execution_start": 1700118992092,
        "source_hash": null,
        "id": "u4832hpdQR7A"
      },
      "outputs": [],
      "source": [
        "def mongoimport(csv_path, db_name, coll_name, db_url):\n",
        "    \"\"\" Imports a csv file at path csv_name to a mongo colection\n",
        "    returns: count of the documants in the new collection\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Connect to MongoDB\n",
        "        client = MongoClient(db_url)\n",
        "        db = client[db_name]\n",
        "        coll = db[coll_name]\n",
        "\n",
        "        # Load the data as a JSON compatible form\n",
        "        data = pd.read_csv(csv_path)\n",
        "        payload = data.to_dict(\"records\")\n",
        "\n",
        "        # Empty collection and upload\n",
        "        coll.drop()\n",
        "        coll.insert_many(payload)\n",
        "\n",
        "        return \"Successful Upload\"\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        return \"Error: {}\".format(str(e))\n",
        "\n",
        "csv_path = \"AllLeadsFiltered.csv\"\n",
        "db_name = \"Lead_Automator_Leads\"\n",
        "coll_name = \"All_Leads_Filtered\"\n",
        "db_url = \"\" # Add your database URL here.\n",
        "\n",
        "response = mongoimport(csv_path, db_name, coll_name, db_url)\n",
        "\n",
        "if response != \"Successful Upload\":\n",
        "\n",
        "  print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFqLv61Vn30w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_full_width": true,
    "deepnote_notebook_id": "d673f2434c324e3a86c5b3c6146e8352",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}